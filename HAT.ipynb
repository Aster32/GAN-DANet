{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 46, 168, 64])\n",
      "Output shape: torch.Size([2, 1, 672, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sun/miniconda3/lib/python3.12/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380481/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Channel Attention Module\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction_ratio=16):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        reduced_channels = max(channels // reduction_ratio, 1)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, reduced_channels, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(reduced_channels, channels, bias=False),\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            b, c, h, w = x.size()\n",
    "            y = self.avg_pool(x).view(b, c)\n",
    "            y = self.fc(y).view(b, c, 1, 1)\n",
    "            y = self.sigmoid(y)\n",
    "            return x * y\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error in ChannelAttention forward pass: {e}\")\n",
    "\n",
    "# Window Attention Module\n",
    "class WindowAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads, window_size):\n",
    "        super(WindowAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.scale = (dim // num_heads) ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "\n",
    "        self.relative_position_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_heads)\n",
    "        )\n",
    "        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "\n",
    "        # Relative position index for each token inside the window\n",
    "        coords_h = torch.arange(self.window_size)\n",
    "        coords_w = torch.arange(self.window_size)\n",
    "        coords = torch.stack(torch.meshgrid(coords_h, coords_w))  # 2, Wh, Ww\n",
    "        coords_flatten = coords.reshape(2, -1)  # 2, Wh*Ww\n",
    "        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, N, N\n",
    "        relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # N, N, 2\n",
    "        relative_coords[:, :, 0] += self.window_size - 1  # Shift to start from 0\n",
    "        relative_coords[:, :, 1] += self.window_size - 1\n",
    "        relative_coords[:, :, 0] *= 2 * self.window_size - 1\n",
    "        relative_position_index = relative_coords.sum(-1)  # N, N\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            B_, N, C = x.shape  # x is of shape (B*num_windows, N, C)\n",
    "            qkv = self.qkv(x).reshape(B_, N, 3, self.num_heads, C // self.num_heads)\n",
    "            qkv = (\n",
    "                qkv.permute(2, 0, 3, 1, 4).contiguous()\n",
    "            )  # (3, B_, num_heads, N, head_dim)\n",
    "            q, k, v = qkv.unbind(0)  # Each has shape (B_, num_heads, N, head_dim)\n",
    "\n",
    "            q = q * self.scale\n",
    "            attn = (q @ k.transpose(-2, -1))  # (B_, num_heads, N, N)\n",
    "\n",
    "            relative_position_bias = self.relative_position_bias_table[\n",
    "                self.relative_position_index.view(-1)\n",
    "            ].view(\n",
    "                N, N, -1\n",
    "            )  # (N, N, num_heads)\n",
    "            relative_position_bias = (\n",
    "                relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "            )  # (num_heads, N, N)\n",
    "            attn = attn + relative_position_bias.unsqueeze(0)\n",
    "            attn = attn.softmax(dim=-1)\n",
    "\n",
    "            x = (attn @ v).transpose(1, 2).reshape(B_, N, C)\n",
    "            x = self.proj(x)\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error in WindowAttention forward pass: {e}\")\n",
    "\n",
    "# Hybrid Attention Block (HAB)\n",
    "class HAB(nn.Module):\n",
    "    def __init__(self, channels, window_size, num_heads):\n",
    "        super(HAB, self).__init__()\n",
    "        self.window_size = window_size\n",
    "        self.norm1 = nn.LayerNorm(channels)\n",
    "        self.channel_attention = ChannelAttention(channels)\n",
    "        self.norm2 = nn.LayerNorm(channels)\n",
    "        self.window_attention = WindowAttention(channels, num_heads, window_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            residual = x  # x: (B, C, H, W)\n",
    "\n",
    "            # LayerNorm and Channel Attention\n",
    "            x = x.permute(0, 2, 3, 1).contiguous()  # (B, H, W, C)\n",
    "            x = self.norm1(x)\n",
    "            x = x.permute(0, 3, 1, 2).contiguous()  # (B, C, H, W)\n",
    "            x = self.channel_attention(x)\n",
    "            x = x + residual  # (B, C, H, W)\n",
    "\n",
    "            residual = x\n",
    "\n",
    "            # LayerNorm and Window Attention\n",
    "            x = x.permute(0, 2, 3, 1).contiguous()  # (B, H, W, C)\n",
    "            x = self.norm2(x)\n",
    "            B, H, W, C = x.shape\n",
    "\n",
    "            # Pad H and W if needed\n",
    "            pad_h = (self.window_size - H % self.window_size) % self.window_size\n",
    "            pad_w = (self.window_size - W % self.window_size) % self.window_size\n",
    "            if pad_h > 0 or pad_w > 0:\n",
    "                x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n",
    "            Hp = H + pad_h\n",
    "            Wp = W + pad_w\n",
    "\n",
    "            # Partition windows\n",
    "            x = x.view(\n",
    "                B,\n",
    "                Hp // self.window_size,\n",
    "                self.window_size,\n",
    "                Wp // self.window_size,\n",
    "                self.window_size,\n",
    "                C,\n",
    "            )\n",
    "            x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "            x = x.view(-1, self.window_size * self.window_size, C)  # (num_windows*B, N, C)\n",
    "\n",
    "            # Window Attention\n",
    "            x = self.window_attention(x)\n",
    "\n",
    "            # Merge windows\n",
    "            x = x.view(\n",
    "                B,\n",
    "                Hp // self.window_size,\n",
    "                Wp // self.window_size,\n",
    "                self.window_size,\n",
    "                self.window_size,\n",
    "                C,\n",
    "            )\n",
    "            x = x.permute(0, 1, 3, 2, 4, 5).contiguous()\n",
    "            x = x.view(B, Hp, Wp, C)\n",
    "\n",
    "            # Remove padding\n",
    "            if pad_h > 0 or pad_w > 0:\n",
    "                x = x[:, :H, :W, :].contiguous()\n",
    "\n",
    "            x = x.permute(0, 3, 1, 2).contiguous()  # (B, C, H, W)\n",
    "            x = x + residual  # (B, C, H, W)\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error in HAB forward pass: {e}\")\n",
    "\n",
    "# Residual Hybrid Attention Group (RHAG)\n",
    "class RHAG(nn.Module):\n",
    "    def __init__(self, channels, num_habs, window_size, num_heads):\n",
    "        super(RHAG, self).__init__()\n",
    "        self.habs = nn.ModuleList(\n",
    "            [HAB(channels, window_size, num_heads) for _ in range(num_habs)]\n",
    "        )\n",
    "        self.conv = nn.Conv2d(\n",
    "            channels, channels, kernel_size=3, stride=1, padding=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            residual = x  # x: (B, C, H, W)\n",
    "\n",
    "            for hab in self.habs:\n",
    "                x = hab(x)  # x remains in shape (B, C, H, W)\n",
    "\n",
    "            x = self.conv(x)  # x: (B, C, H, W)\n",
    "            x = x + residual  # (B, C, H, W)\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error in RHAG forward pass: {e}\")\n",
    "\n",
    "# Full HAT Network Without ConvLSTM\n",
    "class HAT(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels,\n",
    "        out_channels=1,\n",
    "        channels=64,\n",
    "        num_groups=4,\n",
    "        num_habs=6,\n",
    "        window_size=8,\n",
    "        num_heads=8,\n",
    "        upscale_factor=4,\n",
    "        device=None,\n",
    "    ):\n",
    "        super(HAT, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.channels = channels\n",
    "        self.upscale_factor = upscale_factor\n",
    "        self.device = device or torch.device('cpu')\n",
    "\n",
    "        self.entry = nn.Conv2d(\n",
    "            in_channels, channels, kernel_size=3, stride=1, padding=1\n",
    "        ).to(self.device)\n",
    "        self.groups = nn.ModuleList(\n",
    "            [\n",
    "                RHAG(channels, num_habs, window_size, num_heads)\n",
    "                for _ in range(num_groups)\n",
    "            ]\n",
    "        ).to(self.device)\n",
    "        self.conv_after_body = nn.Conv2d(\n",
    "            channels, channels, kernel_size=3, stride=1, padding=1\n",
    "        ).to(self.device)\n",
    "        self.upsample = self._make_upsample_layer()\n",
    "        self.exit = nn.Conv2d(\n",
    "            channels, out_channels, kernel_size=3, stride=1, padding=1\n",
    "        ).to(self.device)\n",
    "\n",
    "        # Adjust residual channels if in_channels != out_channels\n",
    "        if in_channels != out_channels:\n",
    "            self.residual_conv = nn.Conv2d(\n",
    "                in_channels, out_channels, kernel_size=1, stride=1, padding=0\n",
    "            ).to(self.device)\n",
    "        else:\n",
    "            self.residual_conv = nn.Identity()\n",
    "\n",
    "    def _make_upsample_layer(self):\n",
    "        layers = []\n",
    "        num_upsamples = int(self.upscale_factor / 2)\n",
    "        for _ in range(num_upsamples):\n",
    "            layers += [\n",
    "                nn.Conv2d(\n",
    "                    self.channels, self.channels * 4, kernel_size=3, stride=1, padding=1\n",
    "                ),\n",
    "                nn.PixelShuffle(2),  # Upsample x2\n",
    "            ]\n",
    "        return nn.Sequential(*layers).to(self.device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        try:\n",
    "            # x shape: (B, C, H, W)\n",
    "            x = x.to(self.device)\n",
    "            B, C, H, W = x.shape\n",
    "\n",
    "            # Initial residual connection, upscaled input\n",
    "            residual = F.interpolate(\n",
    "                x, scale_factor=self.upscale_factor, mode='bilinear', align_corners=False\n",
    "            )\n",
    "            residual = self.residual_conv(residual)  # Adjust residual channels\n",
    "\n",
    "            x = self.entry(x)  # (B, channels, H, W)\n",
    "            res = x.clone()\n",
    "\n",
    "            for group in self.groups:\n",
    "                x = group(x)  # x: (B, channels, H, W)\n",
    "\n",
    "            x = self.conv_after_body(x)\n",
    "            x = x + res  # Residual connection after body\n",
    "\n",
    "            x = self.upsample(x)  # Upsample x\n",
    "\n",
    "            x = self.exit(x)  # (B, out_channels, H*upscale_factor, W*upscale_factor)\n",
    "\n",
    "            # Ensure shapes match before adding residual\n",
    "            assert x.shape == residual.shape, (\n",
    "                f\"Shape mismatch: x.shape={x.shape}, residual.shape={residual.shape}\"\n",
    "            )\n",
    "\n",
    "            x = x + residual  # Add residual\n",
    "\n",
    "            return x\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error in HAT forward pass: {e}\")\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Data preparation\n",
    "    batch_size = 2  # Number of samples\n",
    "    channels = 46    # Number of input channels\n",
    "    height = 168    # Height\n",
    "    width = 64      # Width\n",
    "\n",
    "    # Simulate data\n",
    "    data = torch.randn(batch_size, channels, height, width).to(device)  # Shape: (B, C, H, W)\n",
    "\n",
    "    # Initialize the model\n",
    "    model = HAT(\n",
    "        in_channels=channels,\n",
    "        out_channels=1,   # Configured number of output channels\n",
    "        channels=64,\n",
    "        num_groups=4,\n",
    "        num_habs=6,\n",
    "        window_size=8,\n",
    "        num_heads=8,\n",
    "        upscale_factor=4,\n",
    "        device=device,\n",
    "    ).to(device)\n",
    "\n",
    "    # Run the model\n",
    "    output = model(data)\n",
    "\n",
    "    print(\"Input shape:\", data.shape)\n",
    "    print(\"Output shape:\", output.shape)  # Should be (B, out_channels, H*4, W*4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
