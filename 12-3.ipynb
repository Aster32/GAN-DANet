{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from model import (\n",
    "    Discriminator1,\n",
    "    HAT,\n",
    "    weights_init_normal,\n",
    "    SSIM,\n",
    "    TVLoss,\n",
    "    PerceptualLoss,\n",
    "    LPIPSNet\n",
    ")\n",
    "from datasets import CustomDataset, load_data_with_augmentation\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        epochs,\n",
    "        batch_size,\n",
    "        smoothing_method=None,\n",
    "        attention_type=None,\n",
    "        senet_module=None,\n",
    "        random_seed=42,\n",
    "        lpips_weights_path=None,\n",
    "        single_channel=True,\n",
    "        config=None\n",
    "    ):\n",
    "        # Default configuration\n",
    "        default_config = {\n",
    "            'lr_D': 0.0004,\n",
    "            'lr_U': 0.0002,\n",
    "            'adversarial_weight_start': 0.0,\n",
    "            'adversarial_weight_end': 1.0,\n",
    "            'lpips_weight': 0.5,\n",
    "            'patience': 30,\n",
    "            'delta': 0.0001,\n",
    "            'lpips_adapt_epochs': 10,\n",
    "            'lpips_margin': 0.5,\n",
    "            'grad_clip': 1.0,\n",
    "            'use_lpips': True,\n",
    "            'use_mixed_precision': True\n",
    "        }\n",
    "\n",
    "        if config is not None:\n",
    "            default_config.update(config)\n",
    "        self.cfg = default_config\n",
    "\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.smoothing_method = smoothing_method\n",
    "        self.attention_type = attention_type\n",
    "        self.senet_module = senet_module\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.random_seed = random_seed\n",
    "        self.single_channel = single_channel\n",
    "        self.lpips_weights_path = lpips_weights_path\n",
    "\n",
    "        # Load and prepare data\n",
    "        (\n",
    "            [self.lr_grace_05, self.trend05],\n",
    "            [self.lr_grace_025, self.trend25],\n",
    "            self.hr_aux,\n",
    "            self.grace_scaler_05,\n",
    "            self.grace_scaler_025,\n",
    "            self.aux_scalers,\n",
    "        ) = load_data_with_augmentation()\n",
    "\n",
    "        # Apply smoothing if specified\n",
    "        if self.smoothing_method:\n",
    "            self.hr_aux = self.smoothing_method(self.hr_aux)\n",
    "\n",
    "        dataset = CustomDataset(self.lr_grace_05, self.lr_grace_025, self.hr_aux)\n",
    "        print(np.shape(self.hr_aux))\n",
    "\n",
    "        # Split dataset into train/val/test (70/15/15)\n",
    "        train_size = int(0.7 * len(dataset))\n",
    "        val_size = int(0.15 * len(dataset))\n",
    "        test_size = len(dataset) - train_size - val_size\n",
    "        self.train_dataset, self.val_dataset, self.test_dataset = random_split(\n",
    "            dataset,\n",
    "            [train_size, val_size, test_size],\n",
    "            generator=torch.Generator().manual_seed(self.random_seed),\n",
    "        )\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "        # Initialize models\n",
    "        self.discriminator = Discriminator1().to(self.device)\n",
    "        self.HAT = HAT(\n",
    "            in_channels=self.hr_aux.shape[-1] + 1,\n",
    "            out_channels=1,\n",
    "            channels=64,\n",
    "            num_groups=4,\n",
    "            num_habs=6,\n",
    "            window_size=8,\n",
    "            num_heads=8,\n",
    "            upscale_factor=4,\n",
    "            device=self.device\n",
    "        ).to(self.device)\n",
    "\n",
    "        if self.senet_module:\n",
    "            self.senet_module = self.senet_module.to(self.device)\n",
    "        else:\n",
    "            self.senet_module = None\n",
    "\n",
    "        # Initialize weights\n",
    "        self.discriminator.apply(weights_init_normal)\n",
    "        self.HAT.apply(weights_init_normal)\n",
    "        if self.senet_module:\n",
    "            self.senet_module.apply(weights_init_normal)\n",
    "\n",
    "        # LPIPS model\n",
    "        self.lpips = LPIPSNet(single_channel=self.single_channel, normalize_inputs=True).to(self.device)\n",
    "        if self.lpips_weights_path:\n",
    "            self.lpips.load_lpips_weights(self.lpips_weights_path)\n",
    "\n",
    "        # Optimizers\n",
    "        hat_parameters = list(self.HAT.parameters())\n",
    "        if self.senet_module:\n",
    "            hat_parameters += list(self.senet_module.parameters())\n",
    "\n",
    "        self.optimizer_D = torch.optim.AdamW(\n",
    "            self.discriminator.parameters(),\n",
    "            lr=self.cfg['lr_D'],\n",
    "            betas=(0.5, 0.999),\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "        self.optimizer_U = torch.optim.AdamW(\n",
    "            hat_parameters,\n",
    "            lr=self.cfg['lr_U'],\n",
    "            betas=(0.5, 0.999),\n",
    "            weight_decay=1e-4,\n",
    "        )\n",
    "\n",
    "        # Schedulers\n",
    "        self.scheduler_D = CosineAnnealingWarmRestarts(self.optimizer_D, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "        self.scheduler_U = CosineAnnealingWarmRestarts(self.optimizer_U, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "\n",
    "        # Loss functions\n",
    "        self.adversarial_loss = nn.BCEWithLogitsLoss()\n",
    "        self.pixelwise_loss = nn.MSELoss()\n",
    "        self.ssim_loss = SSIM(window_size=11, size_average=True).to(self.device)\n",
    "        self.tv_loss = TVLoss(weight=1e-5).to(self.device)\n",
    "        self.perceptual_loss = PerceptualLoss(use_gpu=torch.cuda.is_available())\n",
    "\n",
    "        # Gradient scaler for mixed-precision\n",
    "        self.scaler = GradScaler(enabled=self.cfg['use_mixed_precision'])\n",
    "\n",
    "    def adapt_lpips(self):\n",
    "        \"\"\"\n",
    "        Adapt LPIPS using lr_grace_025 data:\n",
    "        Positive pairs: (x, x)\n",
    "        Negative pairs: (x, degraded_x) where degraded_x is down-up sampled to original size.\n",
    "        \"\"\"\n",
    "        self.lpips.train()\n",
    "        # Freeze backbone, only train lin layers\n",
    "        for p in self.lpips.parameters():\n",
    "            p.requires_grad = False\n",
    "        for lin in self.lpips.lins:\n",
    "            for p in lin.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "        lpips_optimizer = torch.optim.Adam([p for lin in self.lpips.lins for p in lin.parameters()], lr=1e-4)\n",
    "        adapt_loader = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "        margin = self.cfg['lpips_margin']\n",
    "        for ep in range(self.cfg['lpips_adapt_epochs']):\n",
    "            total_loss = 0.0\n",
    "            count = 0\n",
    "            for lr_grace_05, lr_grace_025, hr_aux in adapt_loader:\n",
    "                # lr_grace_025 is your domain data\n",
    "                x = lr_grace_025.to(self.device)\n",
    "\n",
    "                # Positive pair: (x, x)\n",
    "                pos_dist = self.lpips(x, x)\n",
    "\n",
    "                # Negative pair: create degraded_x by downscaling and then upscaling back to original shape\n",
    "                B, C, H, W = x.shape\n",
    "                degraded_x_down = F.interpolate(x, scale_factor=0.5, mode='bicubic', align_corners=False)\n",
    "                degraded_x = F.interpolate(degraded_x_down, size=(H, W), mode='bicubic', align_corners=False)\n",
    "\n",
    "                neg_dist = self.lpips(x, degraded_x)\n",
    "\n",
    "                # Loss: minimize pos_dist and ensure neg_dist > margin\n",
    "                # L = pos_dist + max(0, margin - neg_dist)\n",
    "                loss = pos_dist + F.relu(margin - neg_dist)\n",
    "\n",
    "                lpips_optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                lpips_optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                count += 1\n",
    "\n",
    "            avg_loss = total_loss / max(count, 1)\n",
    "            print(f\"LPIPS Adaptation Epoch [{ep+1}/{self.cfg['lpips_adapt_epochs']}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Freeze LPIPS after adaptation\n",
    "        self.lpips.eval()\n",
    "        for p in self.lpips.parameters():\n",
    "            p.requires_grad = False\n",
    "        print(\"LPIPS adaptation complete. LPIPS is now domain-adjusted.\")\n",
    "\n",
    "    def train_main_model(self):\n",
    "        train_losses_G = []\n",
    "        train_losses_D = []\n",
    "        val_losses_G = []\n",
    "\n",
    "        patience = self.cfg['patience']\n",
    "        delta = self.cfg['delta']\n",
    "        best_loss = float('inf')\n",
    "        trigger_times = 0\n",
    "\n",
    "        hat_parameters = list(self.HAT.parameters())\n",
    "        if self.senet_module:\n",
    "            hat_parameters += list(self.senet_module.parameters())\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss_G = 0.0\n",
    "            epoch_loss_D = 0.0\n",
    "\n",
    "            self.HAT.train()\n",
    "            self.discriminator.train()\n",
    "            if self.senet_module:\n",
    "                self.senet_module.train()\n",
    "\n",
    "            adv_start = self.cfg['adversarial_weight_start']\n",
    "            adv_end = self.cfg['adversarial_weight_end']\n",
    "            adv_weight = adv_start + (adv_end - adv_start) * (epoch / self.epochs)\n",
    "\n",
    "            for lr_grace_05, lr_grace_025, hr_aux in self.train_loader:\n",
    "                lr_grace = F.interpolate(lr_grace_05, scale_factor=0.5, mode='bicubic', align_corners=False).to(self.device)\n",
    "                lr_grace_025 = lr_grace_025.to(self.device)\n",
    "                hr_aux = hr_aux.to(self.device)\n",
    "\n",
    "                downsampled_aux = F.interpolate(hr_aux, scale_factor=0.25, mode='bicubic', align_corners=False)\n",
    "                combined_input = torch.cat([lr_grace, downsampled_aux], dim=1)\n",
    "                if self.senet_module:\n",
    "                    combined_input = self.senet_module(combined_input)\n",
    "\n",
    "                with autocast(enabled=self.cfg['use_mixed_precision']):\n",
    "                    hr_generated = self.HAT(combined_input)\n",
    "\n",
    "                    # Discriminator training\n",
    "                    self.optimizer_D.zero_grad()\n",
    "                    real_output = self.discriminator(lr_grace_025)\n",
    "                    fake_output = self.discriminator(hr_generated.detach())\n",
    "                    real_labels = torch.ones_like(real_output, device=self.device)\n",
    "                    fake_labels = torch.zeros_like(fake_output, device=self.device)\n",
    "\n",
    "                    loss_D_real = self.adversarial_loss(real_output, real_labels)\n",
    "                    loss_D_fake = self.adversarial_loss(fake_output, fake_labels)\n",
    "                    loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "\n",
    "                self.scaler.scale(loss_D).backward()\n",
    "                self.scaler.unscale_(self.optimizer_D)\n",
    "                if self.cfg['grad_clip'] > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(self.discriminator.parameters(), self.cfg['grad_clip'])\n",
    "                self.scaler.step(self.optimizer_D)\n",
    "\n",
    "                with autocast(enabled=self.cfg['use_mixed_precision']):\n",
    "                    # Generator training\n",
    "                    self.optimizer_U.zero_grad()\n",
    "                    fake_output = self.discriminator(hr_generated)\n",
    "                    loss_G_adv = self.adversarial_loss(fake_output, real_labels) * adv_weight\n",
    "                    loss_G_pixel = self.pixelwise_loss(hr_generated, lr_grace_025)\n",
    "                    loss_G_tv = self.tv_loss(hr_generated)\n",
    "                    loss_G_perceptual = self.perceptual_loss(hr_generated, lr_grace_025)\n",
    "                    loss_LPIPS = self.lpips(hr_generated, lr_grace_025) if self.cfg['use_lpips'] else torch.tensor(0.0, device=self.device)\n",
    "\n",
    "                    loss_G = loss_G_pixel + loss_G_perceptual + loss_G_tv + loss_G_adv + self.cfg['lpips_weight'] * loss_LPIPS\n",
    "\n",
    "                self.scaler.scale(loss_G).backward()\n",
    "                self.scaler.unscale_(self.optimizer_U)\n",
    "                if self.cfg['grad_clip'] > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(hat_parameters, self.cfg['grad_clip'])\n",
    "                self.scaler.step(self.optimizer_U)\n",
    "                self.scaler.update()\n",
    "\n",
    "                epoch_loss_G += loss_G.item()\n",
    "                epoch_loss_D += loss_D.item()\n",
    "\n",
    "            avg_epoch_loss_G = epoch_loss_G / len(self.train_loader)\n",
    "            avg_epoch_loss_D = epoch_loss_D / len(self.train_loader)\n",
    "            train_losses_G.append(avg_epoch_loss_G)\n",
    "            train_losses_D.append(avg_epoch_loss_D)\n",
    "\n",
    "            # Validation phase\n",
    "            val_loss_G = 0.0\n",
    "            self.HAT.eval()\n",
    "            if self.senet_module:\n",
    "                self.senet_module.eval()\n",
    "            with torch.no_grad():\n",
    "                for lr_grace_05, lr_grace_025, hr_aux in self.val_loader:\n",
    "                    lr_grace = F.interpolate(\n",
    "                        lr_grace_05, scale_factor=0.5, mode='bicubic', align_corners=False\n",
    "                    ).to(self.device)\n",
    "                    lr_grace_025 = lr_grace_025.to(self.device)\n",
    "                    hr_aux = hr_aux.to(self.device)\n",
    "\n",
    "                    downsampled_aux = F.interpolate(hr_aux, scale_factor=0.25, mode='bicubic', align_corners=False)\n",
    "                    combined_input = torch.cat([lr_grace, downsampled_aux], dim=1)\n",
    "\n",
    "                    if self.senet_module:\n",
    "                        combined_input = self.senet_module(combined_input)\n",
    "\n",
    "                    hr_generated = self.HAT(combined_input)\n",
    "\n",
    "                    loss_G_pixel = self.pixelwise_loss(hr_generated, lr_grace_025)\n",
    "                    loss_G_tv = self.tv_loss(hr_generated)\n",
    "                    loss_G_perceptual = self.perceptual_loss(hr_generated, lr_grace_025)\n",
    "                    val_loss = loss_G_pixel + loss_G_tv + loss_G_perceptual\n",
    "                    if self.cfg['use_lpips']:\n",
    "                        val_loss += self.cfg['lpips_weight'] * self.lpips(hr_generated, lr_grace_025)\n",
    "                    val_loss_G += val_loss.item()\n",
    "\n",
    "            val_loss_G /= len(self.val_loader)\n",
    "\n",
    "            # Early Stopping Check\n",
    "            if val_loss_G < best_loss - delta:\n",
    "                best_loss = val_loss_G\n",
    "                trigger_times = 0\n",
    "                torch.save(self.HAT.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                trigger_times += 1\n",
    "                print(f'EarlyStopping: {trigger_times}/{patience} epochs with no improvement.')\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping triggered.')\n",
    "                    self.HAT.load_state_dict(torch.load('best_model.pth'))\n",
    "                    break\n",
    "\n",
    "            # Update schedulers\n",
    "            self.scheduler_D.step()\n",
    "            self.scheduler_U.step()\n",
    "\n",
    "            print(\n",
    "                f\"Epoch [{epoch+1}/{self.epochs}] | \"\n",
    "                f\"Loss D: {avg_epoch_loss_D:.4f} | \"\n",
    "                f\"Loss G: {avg_epoch_loss_G:.4f} | \"\n",
    "                f\"Val Loss G: {val_loss_G:.4f} | \"\n",
    "                f\"LR_D: {self.optimizer_D.param_groups[0]['lr']:.6f}, \"\n",
    "                f\"LR_U: {self.optimizer_U.param_groups[0]['lr']:.6f}\"\n",
    "            )\n",
    "\n",
    "        if trigger_times < patience:\n",
    "            self.HAT.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "        return train_losses_G, train_losses_D, val_losses_G\n",
    "\n",
    "    def train(self):\n",
    "        # Adapt LPIPS first if single-channel and using LPIPS\n",
    "        if self.single_channel and self.cfg['use_lpips']:\n",
    "            print(\"Adapting LPIPS to domain-specific single-channel lr_grace_025 data...\")\n",
    "            self.adapt_lpips()\n",
    "        return self.train_main_model()\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.HAT.eval()\n",
    "        if self.senet_module:\n",
    "            self.senet_module.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            trues = []\n",
    "            for lr_grace_05, lr_grace_025, hr_aux in self.test_loader:\n",
    "                lr_grace = F.interpolate(\n",
    "                    lr_grace_05, scale_factor=0.5, mode='bicubic', align_corners=False\n",
    "                ).to(self.device)\n",
    "                lr_grace_025 = lr_grace_025.to(self.device)\n",
    "                hr_aux = hr_aux.to(self.device)\n",
    "\n",
    "                downsampled_aux = F.interpolate(\n",
    "                    hr_aux, scale_factor=0.25, mode='bicubic', align_corners=False\n",
    "                )\n",
    "                combined_input = torch.cat([lr_grace, downsampled_aux], dim=1)\n",
    "\n",
    "                if self.senet_module:\n",
    "                    combined_input = self.senet_module(combined_input)\n",
    "\n",
    "                hr_generated = self.HAT(combined_input)\n",
    "\n",
    "                preds.append(hr_generated.cpu().numpy())\n",
    "                trues.append(lr_grace_025.cpu().numpy())\n",
    "\n",
    "            preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "            trues = np.concatenate(trues, axis=0).reshape(-1)\n",
    "\n",
    "            cc = np.corrcoef(trues, preds)[0, 1]\n",
    "            mse = mean_squared_error(trues, preds)\n",
    "            mae = mean_absolute_error(trues, preds)\n",
    "            r2 = r2_score(trues, preds)\n",
    "\n",
    "            print(\n",
    "                f\"Test MSE: {mse:.6f}, Test MAE: {mae:.6f}, Test R²: {r2:.6f}, Test CC: {cc:.6f}\"\n",
    "            )\n",
    "\n",
    "        return preds, trues, r2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181, 90, 44)\n",
      "(181, 180, 88, 1)\n",
      "[509.70157107]\n",
      "[-32767.]\n",
      "[-32767.]\n",
      "[-32767.]\n",
      "Combined HR Aux Data Shape: (181, 180, 88, 45)\n",
      "0.0\n",
      "65.5\n",
      "Sliced HR Aux Data Shape: (181, 180, 88, 45)\n",
      "-5.350948318234112\n",
      "(180, 88, 7)\n",
      "最大误差: 8.881784197001252e-16\n",
      "最大误差: 8.881784197001252e-16\n",
      "(543, 180, 88, 45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xy/miniconda3/envs/xy/lib/python3.12/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/home/xy/miniconda3/envs/xy/lib/python3.12/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1711403380481/work/aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/home/xy/miniconda3/envs/xy/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/xy/miniconda3/envs/xy/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/home/xy/miniconda3/envs/xy/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adapting LPIPS to domain-specific single-channel lr_grace_025 data...\n",
      "LPIPS Adaptation Epoch [1/10], Loss: 0.4899\n",
      "LPIPS Adaptation Epoch [2/10], Loss: 0.4852\n",
      "LPIPS Adaptation Epoch [3/10], Loss: 0.4806\n",
      "LPIPS Adaptation Epoch [4/10], Loss: 0.4759\n",
      "LPIPS Adaptation Epoch [5/10], Loss: 0.4712\n",
      "LPIPS Adaptation Epoch [6/10], Loss: 0.4665\n",
      "LPIPS Adaptation Epoch [7/10], Loss: 0.4619\n",
      "LPIPS Adaptation Epoch [8/10], Loss: 0.4572\n",
      "LPIPS Adaptation Epoch [9/10], Loss: 0.4525\n",
      "LPIPS Adaptation Epoch [10/10], Loss: 0.4478\n",
      "LPIPS adaptation complete. LPIPS is now domain-adjusted.\n",
      "Epoch [1/1] | Loss D: 0.6720 | Loss G: 2.9749 | Val Loss G: 2.7114 | LR_D: 0.000390, LR_U: 0.000195\n",
      "(16, 1, 180, 88)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid shape (16, 1, 180, 88) for image data",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m model1 \u001b[38;5;241m=\u001b[39m ModelTrainer(epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, smoothing_method\u001b[38;5;241m=\u001b[39msmoothing_method)\n\u001b[1;32m     19\u001b[0m train_losses_G1, train_losses_D1, val_losses_G\u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 20\u001b[0m preds1, trues1, r2_1 \u001b[38;5;241m=\u001b[39m model1\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m     21\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
      "Cell \u001b[0;32mIn[1], line 404\u001b[0m, in \u001b[0;36mModelTrainer.evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(pred_array)) \u001b[38;5;66;03m# Extract the actual array\u001b[39;00m\n\u001b[1;32m    403\u001b[0m pred_array\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39msqueeze(pred_array)\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_plot_spatial_distribution(pred_array, trues)\n\u001b[1;32m    405\u001b[0m preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(preds, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    406\u001b[0m trues \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(trues, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 441\u001b[0m, in \u001b[0;36mModelTrainer._plot_spatial_distribution\u001b[0;34m(self, predicted, target)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;66;03m# Plot true\u001b[39;00m\n\u001b[1;32m    440\u001b[0m plt\u001b[38;5;241m.\u001b[39msubplot(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 441\u001b[0m im2 \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mimshow(true_sample, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjet_r\u001b[39m\u001b[38;5;124m'\u001b[39m, vmin\u001b[38;5;241m=\u001b[39mvmin, vmax\u001b[38;5;241m=\u001b[39mvmax)\n\u001b[1;32m    442\u001b[0m plt\u001b[38;5;241m.\u001b[39mcolorbar(im2, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue GRACE Value\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    443\u001b[0m plt\u001b[38;5;241m.\u001b[39mcontour(true_mask, levels\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m], colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblack\u001b[39m\u001b[38;5;124m'\u001b[39m, linewidths\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/xy/lib/python3.12/site-packages/matplotlib/pyplot.py:3601\u001b[0m, in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   3579\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mimshow)\n\u001b[1;32m   3580\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimshow\u001b[39m(\n\u001b[1;32m   3581\u001b[0m     X: ArrayLike \u001b[38;5;241m|\u001b[39m PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3599\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3600\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AxesImage:\n\u001b[0;32m-> 3601\u001b[0m     __ret \u001b[38;5;241m=\u001b[39m gca()\u001b[38;5;241m.\u001b[39mimshow(\n\u001b[1;32m   3602\u001b[0m         X,\n\u001b[1;32m   3603\u001b[0m         cmap\u001b[38;5;241m=\u001b[39mcmap,\n\u001b[1;32m   3604\u001b[0m         norm\u001b[38;5;241m=\u001b[39mnorm,\n\u001b[1;32m   3605\u001b[0m         aspect\u001b[38;5;241m=\u001b[39maspect,\n\u001b[1;32m   3606\u001b[0m         interpolation\u001b[38;5;241m=\u001b[39minterpolation,\n\u001b[1;32m   3607\u001b[0m         alpha\u001b[38;5;241m=\u001b[39malpha,\n\u001b[1;32m   3608\u001b[0m         vmin\u001b[38;5;241m=\u001b[39mvmin,\n\u001b[1;32m   3609\u001b[0m         vmax\u001b[38;5;241m=\u001b[39mvmax,\n\u001b[1;32m   3610\u001b[0m         colorizer\u001b[38;5;241m=\u001b[39mcolorizer,\n\u001b[1;32m   3611\u001b[0m         origin\u001b[38;5;241m=\u001b[39morigin,\n\u001b[1;32m   3612\u001b[0m         extent\u001b[38;5;241m=\u001b[39mextent,\n\u001b[1;32m   3613\u001b[0m         interpolation_stage\u001b[38;5;241m=\u001b[39minterpolation_stage,\n\u001b[1;32m   3614\u001b[0m         filternorm\u001b[38;5;241m=\u001b[39mfilternorm,\n\u001b[1;32m   3615\u001b[0m         filterrad\u001b[38;5;241m=\u001b[39mfilterrad,\n\u001b[1;32m   3616\u001b[0m         resample\u001b[38;5;241m=\u001b[39mresample,\n\u001b[1;32m   3617\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   3618\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m: data} \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[1;32m   3619\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3620\u001b[0m     )\n\u001b[1;32m   3621\u001b[0m     sci(__ret)\n\u001b[1;32m   3622\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m __ret\n",
      "File \u001b[0;32m~/miniconda3/envs/xy/lib/python3.12/site-packages/matplotlib/__init__.py:1521\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1520\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1521\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\n\u001b[1;32m   1522\u001b[0m             ax,\n\u001b[1;32m   1523\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(cbook\u001b[38;5;241m.\u001b[39msanitize_sequence, args),\n\u001b[1;32m   1524\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{k: cbook\u001b[38;5;241m.\u001b[39msanitize_sequence(v) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems()})\n\u001b[1;32m   1526\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1527\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1528\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m~/miniconda3/envs/xy/lib/python3.12/site-packages/matplotlib/axes/_axes.py:5979\u001b[0m, in \u001b[0;36mAxes.imshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, colorizer, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5976\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aspect \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5977\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_aspect(aspect)\n\u001b[0;32m-> 5979\u001b[0m im\u001b[38;5;241m.\u001b[39mset_data(X)\n\u001b[1;32m   5980\u001b[0m im\u001b[38;5;241m.\u001b[39mset_alpha(alpha)\n\u001b[1;32m   5981\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mget_clip_path() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5982\u001b[0m     \u001b[38;5;66;03m# image does not already have clipping set, clip to Axes patch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xy/lib/python3.12/site-packages/matplotlib/image.py:685\u001b[0m, in \u001b[0;36m_ImageBase.set_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    683\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(A, PIL\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mImage):\n\u001b[1;32m    684\u001b[0m     A \u001b[38;5;241m=\u001b[39m pil_to_array(A)  \u001b[38;5;66;03m# Needed e.g. to apply png palette.\u001b[39;00m\n\u001b[0;32m--> 685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_normalize_image_array(A)\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_imcache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/xy/lib/python3.12/site-packages/matplotlib/image.py:653\u001b[0m, in \u001b[0;36m_ImageBase._normalize_image_array\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    651\u001b[0m     A \u001b[38;5;241m=\u001b[39m A\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# If just (M, N, 1), assume scalar and apply colormap.\u001b[39;00m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m A\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m]):\n\u001b[0;32m--> 653\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mA\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for image data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m A\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;66;03m# If the input data has values outside the valid range (after\u001b[39;00m\n\u001b[1;32m    656\u001b[0m     \u001b[38;5;66;03m# normalisation), we issue a warning and then clip X to the bounds\u001b[39;00m\n\u001b[1;32m    657\u001b[0m     \u001b[38;5;66;03m# - otherwise casting wraps extreme values, hiding outliers and\u001b[39;00m\n\u001b[1;32m    658\u001b[0m     \u001b[38;5;66;03m# making reliable interpretation impossible.\u001b[39;00m\n\u001b[1;32m    659\u001b[0m     high \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39missubdtype(A\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid shape (16, 1, 180, 88) for image data"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "epochs = 1\n",
    "batch_size = 16\n",
    "# Instantiate the modu\n",
    "\n",
    "\n",
    "# Define smoothing method\n",
    "#smoothing_method = ModelTrainer(epochs, batch_size, 1024).smooth_data_gaussian\n",
    "smoothing_method = None\n",
    "# Define modules\n",
    "#attention_module = AttentionModule(input_channels=40, output_channels=40)\n",
    "#senet_module = SqueezeExcitation(input_channels=40, reduction_ratio=8)\n",
    "\n",
    "# Train the baseline model without any additional module\n",
    "# Release GPU memory\n",
    "\n",
    "# Train the model with Attention\n",
    "model1 = ModelTrainer(epochs=epochs, batch_size=batch_size, smoothing_method=smoothing_method)\n",
    "train_losses_G1, train_losses_D1, val_losses_G= model1.train()\n",
    "preds1, trues1, r2_1 = model1.evaluate()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.HAT.state_dict(), 'model11_HAT.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
