{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from model import OriginalRelationshipLearner, Discriminator1, FlexibleUpsamplingModule, weights_init_normal, SSIM, TVLoss, PerceptualLoss\n",
    "from datasets import CustomDataset, load_data_with_augmentation\n",
    "import torch.nn.functional as F\n",
    "from utils import plot_results\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage import gaussian_filter, median_filter\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from taylorDiagram import TaylorDiagram\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import copy\n",
    "def visualize_model(model, filename,x):\n",
    "    #x = torch.randn(input_size)\n",
    "    y = model(x)\n",
    "    dot = make_dot(y, params=dict(model.named_parameters()), show_attrs=True, show_saved=True)\n",
    "    dot.format = 'png'\n",
    "    dot.render(filename, cleanup=True)\n",
    "    print(f\"Model architecture saved as '{filename}.png'\")\n",
    "class ModelTrainer:\n",
    "    def __init__(self, epochs, batch_size, relationship_learner, relationship_output_channels, smoothing_method=None, attention=None, senet=None, rand=42):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        #self.relationship_learner = relationship_learner\n",
    "        self.relationship_output_channels = relationship_output_channels\n",
    "        self.smoothing_method = smoothing_method\n",
    "        self.attention = attention\n",
    "        self.senet = senet\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.rand=rand\n",
    "        # Load and prepare data\n",
    "        [self.lr_grace_05,self.trend05], [self.lr_grace_025,self.trend25], self.hr_aux, self.grace_scaler_05, self.grace_scaler_025, self.aux_scalers = load_data_with_augmentation()\n",
    "        \n",
    "        # Apply data smoothing to hr_aux if smoothing_method is specified\n",
    "        if self.smoothing_method:\n",
    "            self.hr_aux = self.smoothing_method(self.hr_aux)\n",
    "        else:\n",
    "            self.hr_aux = self.hr_aux\n",
    "        \n",
    "        # Split data into training and testing sets\n",
    "        # Ensure data is sorted by time before splitting\n",
    "        split_index = int(len(self.lr_grace_05) * 0.8)  # 80% training, 20% testing\n",
    "        self.train_lr_grace_05, self.test_lr_grace_05 = self.lr_grace_05[:split_index], self.lr_grace_05[split_index:]\n",
    "        self.train_lr_grace_025, self.test_lr_grace_025 = self.lr_grace_025[:split_index], self.lr_grace_025[split_index:]\n",
    "        self.train_hr_aux, self.test_hr_aux = self.hr_aux[:split_index], self.hr_aux[split_index:]\n",
    "        self.train_lr_grace_05, self.test_lr_grace_05, self.train_lr_grace_025, self.test_lr_grace_025, self.train_hr_aux, self.test_hr_aux = train_test_split(\n",
    "            self.lr_grace_05, self.lr_grace_025, self.hr_aux, test_size=0.2, random_state=self.rand)\n",
    "        \n",
    "        # Create datasets and dataloaders\n",
    "        self.train_dataset = CustomDataset(self.train_lr_grace_05, self.train_lr_grace_025, self.train_hr_aux)\n",
    "        self.test_dataset = CustomDataset(self.test_lr_grace_05, self.test_lr_grace_025, self.test_hr_aux)\n",
    "        \n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "        \n",
    "        # Initialize models\n",
    "        #self.relationship_learner = self.relationship_learner.to(self.device)\n",
    "        self.discriminator = Discriminator1().to(self.device)\n",
    "        self.upsampling_module = FlexibleUpsamplingModule(input_channels=self.hr_aux.shape[-1]+1,attention_type=self.attention).to(self.device)\n",
    "        \n",
    "        self.flag=self.attention\n",
    "        self.attention=None\n",
    "        # Initialize optional modules\n",
    "        if self.attention:\n",
    "            self.attention_module = self.attention.to(self.device)\n",
    "        else:\n",
    "            self.attention_module = None\n",
    "            \n",
    "        if self.senet:\n",
    "            self.senet_module = self.senet.to(self.device)\n",
    "        else:\n",
    "            self.senet_module = None\n",
    "        \n",
    "        # Initialize weights\n",
    "        #self.relationship_learner.apply(weights_init_normal)\n",
    "        self.discriminator.apply(weights_init_normal)\n",
    "        self.upsampling_module.apply(weights_init_normal)\n",
    "        if self.attention_module:\n",
    "            self.attention_module.apply(weights_init_normal)\n",
    "        if self.senet_module:\n",
    "            self.senet_module.apply(weights_init_normal)\n",
    "        \n",
    "        # Optimizers\n",
    "        #self.optimizer_RL = optim.Adam(self.relationship_learner.parameters(), lr=0.0002)\n",
    "        hat_parameters = list(self.upsampling_module.parameters())\n",
    "\n",
    "        if self.attention_module:\n",
    "            hat_parameters += list(self.attention_module.parameters())\n",
    "\n",
    "        if self.senet_module:\n",
    "            hat_parameters += list(self.senet_module.parameters())\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer_D = optim.AdamW(self.discriminator.parameters(), lr=0.0004, betas=(0.5, 0.999), weight_decay=1e-4)\n",
    "        self.optimizer_U = optim.AdamW(hat_parameters, lr=0.0002, betas=(0.5, 0.999), weight_decay=1e-4)\n",
    "\n",
    "        # Learning Rate Schedulers\n",
    "        self.scheduler_D = CosineAnnealingWarmRestarts(self.optimizer_D, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "        self.scheduler_U = CosineAnnealingWarmRestarts(self.optimizer_U, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "        \n",
    "        # Loss functions\n",
    "        self.adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        self.pixelwise_loss = torch.nn.MSELoss()\n",
    "        self.ssim_loss = SSIM(window_size=11, size_average=True).to(self.device)\n",
    "        self.tv_loss = TVLoss(weight=1e-5).to(self.device)\n",
    "        self.perceptual_loss = PerceptualLoss(use_gpu=torch.cuda.is_available())\n",
    "        #self.perceptual_loss = PerceptualLoss([1, 6, 11, 20], use_gpu=torch.cuda.is_available())\n",
    "    def smooth_data_gaussian(self, data, sigma=2):\n",
    "        return gaussian_filter(data, sigma=sigma)\n",
    "\n",
    "    def smooth_data_median(self, data, size=3):\n",
    "        return median_filter(data, size=size)\n",
    "\n",
    "    def smooth_data_savitzky_golay(self, data, window_length=5, polyorder=2):\n",
    "        return savgol_filter(data, window_length, polyorder)\n",
    "\n",
    "    def train(self):\n",
    "        train_losses_G = []\n",
    "        train_losses_D = []\n",
    "        patience = 20  # Early stopping patience\n",
    "        min_delta = 0  # Minimum change in monitored value to qualify as improvement\n",
    "        trigger_times = 0  # Counter for early stopping\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss_G = 0\n",
    "            epoch_loss_D = 0\n",
    "\n",
    "            # Training phase\n",
    "            self.upsampling_module.train()\n",
    "            self.discriminator.train()\n",
    "            if self.attention_module:\n",
    "                self.attention_module.train()\n",
    "            if self.senet_module:\n",
    "                self.senet_module.train()\n",
    "\n",
    "            for lr_grace_05, lr_grace_025, hr_aux in self.train_loader:\n",
    "                lr_grace = F.interpolate(lr_grace_05, scale_factor=0.5, mode='bicubic', align_corners=False)\n",
    "                lr_grace, hr_aux = lr_grace.to(self.device), hr_aux.to(self.device)\n",
    "                lr_grace_025 = lr_grace_025.to(self.device)\n",
    "\n",
    "                # Combine lr_grace and downsampled hr_aux\n",
    "                downsampled_aux = F.interpolate(hr_aux, scale_factor=0.25, mode='bicubic', align_corners=False)\n",
    "                combined_input = torch.cat([lr_grace, downsampled_aux], dim=1)\n",
    "\n",
    "                # Learn relationship features\n",
    "                relationship_features = combined_input\n",
    "                # Apply attention or SENet if exists\n",
    "                if self.attention_module:\n",
    "                    relationship_features = self.attention_module(relationship_features)\n",
    "                elif self.senet_module:\n",
    "                    relationship_features = self.senet_module(relationship_features)\n",
    "\n",
    "                # Generate HR result using HAT module\n",
    "                hr_generated = self.upsampling_module(relationship_features)\n",
    "\n",
    "                # Discriminator training\n",
    "                self.optimizer_D.zero_grad()\n",
    "                real_output = self.discriminator(lr_grace_025)\n",
    "                fake_output = self.discriminator(hr_generated.detach())\n",
    "                real_labels = torch.ones_like(real_output, device=self.device)\n",
    "                fake_labels = torch.zeros_like(fake_output, device=self.device)\n",
    "\n",
    "                loss_D_real = self.adversarial_loss(real_output, real_labels)\n",
    "                loss_D_fake = self.adversarial_loss(fake_output, fake_labels)\n",
    "                loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "                loss_D.backward()\n",
    "                self.optimizer_D.step()\n",
    "\n",
    "                # Generator training\n",
    "                self.optimizer_U.zero_grad()\n",
    "                fake_output = self.discriminator(hr_generated)\n",
    "                loss_G_adv = self.adversarial_loss(fake_output, real_labels)\n",
    "                loss_G_pixel = self.pixelwise_loss(hr_generated, lr_grace_025)\n",
    "                loss_G_ssim = 1 - self.ssim_loss(hr_generated, lr_grace_025)\n",
    "                loss_G_tv = self.tv_loss(hr_generated)\n",
    "                loss_G_perceptual = self.perceptual_loss(hr_generated, lr_grace_025)\n",
    "                loss_weight = epoch / self.epochs  # Linearly increase adversarial weight  HAT\n",
    "                loss_G = (1 - loss_weight) * loss_G_pixel + loss_weight * loss_G_adv + loss_G_tv + loss_G_perceptual\n",
    "                loss_G.backward()\n",
    "                self.optimizer_U.step()\n",
    "\n",
    "                epoch_loss_G += loss_G.item()\n",
    "                epoch_loss_D += loss_D.item()\n",
    "\n",
    "            # Average losses over the epoch\n",
    "            avg_epoch_loss_G = epoch_loss_G / len(self.train_loader)\n",
    "            avg_epoch_loss_D = epoch_loss_D / len(self.train_loader)\n",
    "\n",
    "            # Early Stopping Check\n",
    "            if avg_epoch_loss_G < best_loss - min_delta:\n",
    "                best_loss = avg_epoch_loss_G\n",
    "                trigger_times = 0\n",
    "                # Optionally save the best model state\n",
    "                torch.save(self.upsampling_module.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                trigger_times += 1\n",
    "                print(f'EarlyStopping: {trigger_times}/{patience} epochs with no improvement.')\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping triggered.')\n",
    "                    # Load the best model state before stopping\n",
    "                    self.upsampling_module.load_state_dict(torch.load('best_model.pth'))\n",
    "                    return train_losses_G, train_losses_D\n",
    "\n",
    "            # Update the schedulers at the end of the epoch\n",
    "            self.scheduler_D.step()\n",
    "            self.scheduler_U.step()\n",
    "            if self.attention_module:\n",
    "                self.scheduler_A.step()\n",
    "            if self.senet_module:\n",
    "                self.scheduler_SE.step()\n",
    "\n",
    "            train_losses_G.append(avg_epoch_loss_G)\n",
    "            train_losses_D.append(avg_epoch_loss_D)\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{self.epochs}], Loss D: {avg_epoch_loss_D:.4f}, Loss G: {avg_epoch_loss_G:.4f}')\n",
    "\n",
    "        # Load the best model at the end of training\n",
    "        self.upsampling_module.load_state_dict(torch.load('best_model.pth'))\n",
    "        return train_losses_G, train_losses_D\n",
    "\n",
    "    def evaluate(self):\n",
    "       # self.relationship_learner.eval()\n",
    "        self.upsampling_module.eval()\n",
    "        if self.attention_module:\n",
    "            self.attention_module.eval()\n",
    "        if self.senet_module:\n",
    "            self.senet_module.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            trues = []\n",
    "            bs=0\n",
    "            for lr_grace_05, lr_grace_025, hr_aux in self.test_loader:\n",
    "                \n",
    "                bs=bs+1\n",
    "                if bs==-1 :\n",
    "                    lr_grace_05, lr_grace_025, hr_aux = lr_grace_05.to(self.device), lr_grace_025.to(self.device), hr_aux.to(self.device)\n",
    "                    \n",
    "                    # Combine lr_grace and downsampled hr_aux\n",
    "                    combined_input = torch.cat([lr_grace_025, hr_aux], dim=1)\n",
    "\n",
    "                    # Learn relationship features\n",
    "                    #relationship_features = self.relationship_learner(combined_input)\n",
    "                    relationship_features = combined_input\n",
    "                    # Apply attention or SENet if exists\n",
    "                    if self.attention_module:\n",
    "                        relationship_features = self.attention_module(relationship_features)\n",
    "                    elif self.senet_module:\n",
    "                        relationship_features = self.senet_module(relationship_features)\n",
    "\n",
    "                    # Generate HR result using improved upsampling module\n",
    "                    hr_generated = self.upsampling_module(relationship_features)\n",
    "\n",
    "                    plot_results(lr_grace_05[0,0].cpu(), hr_generated[0,0].cpu(), lr_grace_025[0,0].cpu(), True)\n",
    "                # Save predictions and true values for metrics calculation\n",
    "                lr_grace_05, lr_grace_025, hr_aux = lr_grace_05.to(self.device), lr_grace_025.to(self.device), hr_aux.to(self.device)\n",
    "                lr_grace = F.interpolate(lr_grace_05, scale_factor=0.5, mode='bicubic', align_corners=False)\n",
    "                \n",
    "                # Combine lr_grace and downsampled hr_aux\n",
    "                downsampled_aux = F.interpolate(hr_aux, scale_factor=0.25, mode='bicubic', align_corners=False)\n",
    "                combined_input = torch.cat([lr_grace, downsampled_aux], dim=1)\n",
    "\n",
    "                # Learn relationship features\n",
    "                #relationship_features = self.relationship_learner(combined_input)\n",
    "                relationship_features = combined_input\n",
    "                # Apply attention or SENet if exists\n",
    "                if self.attention_module:\n",
    "                    relationship_features = self.attention_module(relationship_features)\n",
    "                elif self.senet_module:\n",
    "                    relationship_features = self.senet_module(relationship_features)\n",
    "\n",
    "                # Generate HR result using improved upsampling module\n",
    "                hr_generated = self.upsampling_module(relationship_features)\n",
    "\n",
    "                # Upsample lr_grace to create the ground truth for hr_generated\n",
    "                hr_grace_upsampled = lr_grace_025\n",
    "                preds.append(hr_generated.cpu().numpy())\n",
    "                trues.append(hr_grace_upsampled.cpu().numpy())\n",
    "\n",
    "            # Compute evaluation metrics\n",
    "            preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "            trues = np.concatenate(trues, axis=0).reshape(-1)\n",
    "\n",
    "            cc=np.corrcoef(trues, preds)\n",
    "            mse = mean_squared_error(trues, preds)\n",
    "            mae = mean_absolute_error(trues, preds)\n",
    "            r2 = r2_score(trues, preds)\n",
    "\n",
    "            print(f\"Test MSE: {mse}, Test MAE: {mae}, Test R²: {r2}, Test cc: {cc}\")\n",
    "\n",
    "        return preds, trues, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181, 90, 44)\n",
      "(181, 180, 88, 1)\n",
      "[509.70157107]\n",
      "[-32767.]\n",
      "[-32767.]\n",
      "[-32767.]\n",
      "Combined HR Aux Data Shape: (181, 180, 88, 45)\n",
      "0.0\n",
      "65.5\n",
      "Sliced HR Aux Data Shape: (181, 180, 88, 45)\n",
      "-5.350948318234112\n",
      "(180, 88, 7)\n",
      "最大误差: 8.881784197001252e-16\n",
      "最大误差: 8.881784197001252e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sun/miniconda3/lib/python3.12/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/home/sun/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sun/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181, 90, 44)\n",
      "(181, 180, 88, 1)\n",
      "[509.70157107]\n",
      "[-32767.]\n",
      "[-32767.]\n",
      "[-32767.]\n",
      "Combined HR Aux Data Shape: (181, 180, 88, 45)\n",
      "0.0\n",
      "65.5\n",
      "Sliced HR Aux Data Shape: (181, 180, 88, 45)\n",
      "-5.350948318234112\n",
      "(180, 88, 7)\n",
      "最大误差: 8.881784197001252e-16\n",
      "最大误差: 8.881784197001252e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sun/miniconda3/lib/python3.12/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/home/sun/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sun/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss D: 0.5887, Loss G: 2.6316\n",
      "Epoch [2/150], Loss D: 0.4230, Loss G: 2.3873\n",
      "Epoch [3/150], Loss D: 0.4200, Loss G: 2.3094\n",
      "Epoch [4/150], Loss D: 0.4350, Loss G: 2.2421\n",
      "Epoch [5/150], Loss D: 0.4683, Loss G: 2.1854\n",
      "Epoch [6/150], Loss D: 0.5245, Loss G: 2.1262\n",
      "Epoch [7/150], Loss D: 0.5506, Loss G: 2.0773\n",
      "Epoch [8/150], Loss D: 0.5698, Loss G: 2.0359\n",
      "Epoch [9/150], Loss D: 0.5864, Loss G: 2.0074\n",
      "Epoch [10/150], Loss D: 0.6030, Loss G: 1.9951\n",
      "EarlyStopping: 1/20 epochs with no improvement.\n",
      "Epoch [11/150], Loss D: 0.5874, Loss G: 2.1704\n",
      "EarlyStopping: 2/20 epochs with no improvement.\n",
      "Epoch [12/150], Loss D: 0.5813, Loss G: 2.1146\n",
      "EarlyStopping: 3/20 epochs with no improvement.\n",
      "Epoch [13/150], Loss D: 0.5753, Loss G: 2.0589\n",
      "EarlyStopping: 4/20 epochs with no improvement.\n",
      "Epoch [14/150], Loss D: 0.5713, Loss G: 2.0238\n",
      "EarlyStopping: 5/20 epochs with no improvement.\n",
      "Epoch [15/150], Loss D: 0.5635, Loss G: 1.9998\n",
      "Epoch [16/150], Loss D: 0.5714, Loss G: 1.9694\n",
      "Epoch [17/150], Loss D: 0.5906, Loss G: 1.9402\n",
      "Epoch [18/150], Loss D: 0.5938, Loss G: 1.9089\n",
      "Epoch [19/150], Loss D: 0.6027, Loss G: 1.8812\n",
      "Epoch [20/150], Loss D: 0.5951, Loss G: 1.8396\n",
      "Epoch [21/150], Loss D: 0.5904, Loss G: 1.8173\n",
      "Epoch [22/150], Loss D: 0.5939, Loss G: 1.7898\n",
      "Epoch [23/150], Loss D: 0.5967, Loss G: 1.7617\n",
      "Epoch [24/150], Loss D: 0.5986, Loss G: 1.7424\n",
      "Epoch [25/150], Loss D: 0.6006, Loss G: 1.7255\n",
      "Epoch [26/150], Loss D: 0.6026, Loss G: 1.7116\n",
      "Epoch [27/150], Loss D: 0.6073, Loss G: 1.7009\n",
      "Epoch [28/150], Loss D: 0.6132, Loss G: 1.6945\n",
      "Epoch [29/150], Loss D: 0.6193, Loss G: 1.6912\n",
      "EarlyStopping: 1/20 epochs with no improvement.\n",
      "Epoch [30/150], Loss D: 0.6258, Loss G: 1.6913\n",
      "EarlyStopping: 2/20 epochs with no improvement.\n",
      "Epoch [31/150], Loss D: 0.6451, Loss G: 2.0281\n",
      "EarlyStopping: 3/20 epochs with no improvement.\n",
      "Epoch [32/150], Loss D: 0.6029, Loss G: 1.9818\n",
      "EarlyStopping: 4/20 epochs with no improvement.\n",
      "Epoch [33/150], Loss D: 0.5986, Loss G: 1.9997\n",
      "EarlyStopping: 5/20 epochs with no improvement.\n",
      "Epoch [34/150], Loss D: 0.6018, Loss G: 1.9674\n",
      "EarlyStopping: 6/20 epochs with no improvement.\n",
      "Epoch [35/150], Loss D: 0.5967, Loss G: 1.9584\n",
      "EarlyStopping: 7/20 epochs with no improvement.\n",
      "Epoch [36/150], Loss D: 0.5723, Loss G: 1.9669\n",
      "EarlyStopping: 8/20 epochs with no improvement.\n",
      "Epoch [37/150], Loss D: 0.5842, Loss G: 2.0143\n",
      "EarlyStopping: 9/20 epochs with no improvement.\n",
      "Epoch [38/150], Loss D: 0.5921, Loss G: 1.9601\n",
      "EarlyStopping: 10/20 epochs with no improvement.\n",
      "Epoch [39/150], Loss D: 0.5744, Loss G: 1.9554\n",
      "EarlyStopping: 11/20 epochs with no improvement.\n",
      "Epoch [40/150], Loss D: 0.5734, Loss G: 2.0092\n",
      "EarlyStopping: 12/20 epochs with no improvement.\n",
      "Epoch [41/150], Loss D: 0.5839, Loss G: 1.9547\n",
      "EarlyStopping: 13/20 epochs with no improvement.\n",
      "Epoch [42/150], Loss D: 0.5811, Loss G: 1.9784\n",
      "EarlyStopping: 14/20 epochs with no improvement.\n",
      "Epoch [43/150], Loss D: 0.5706, Loss G: 1.9453\n",
      "EarlyStopping: 15/20 epochs with no improvement.\n",
      "Epoch [44/150], Loss D: 0.5644, Loss G: 1.9685\n",
      "EarlyStopping: 16/20 epochs with no improvement.\n",
      "Epoch [45/150], Loss D: 0.5707, Loss G: 1.9793\n",
      "EarlyStopping: 17/20 epochs with no improvement.\n",
      "Epoch [46/150], Loss D: 0.5715, Loss G: 1.9610\n",
      "EarlyStopping: 18/20 epochs with no improvement.\n",
      "Epoch [47/150], Loss D: 0.5863, Loss G: 1.9594\n",
      "EarlyStopping: 19/20 epochs with no improvement.\n",
      "Epoch [48/150], Loss D: 0.5643, Loss G: 1.9383\n",
      "EarlyStopping: 20/20 epochs with no improvement.\n",
      "Early stopping triggered.\n",
      "Test MSE: 0.044227249920368195, Test MAE: 0.1323457658290863, Test R²: 0.9152660687796874, Test cc: [[1.        0.9567689]\n",
      " [0.9567689 1.       ]]\n",
      "(181, 90, 44)\n",
      "(181, 180, 88, 1)\n",
      "[509.70157107]\n",
      "[-32767.]\n",
      "[-32767.]\n",
      "[-32767.]\n",
      "Combined HR Aux Data Shape: (181, 180, 88, 45)\n",
      "0.0\n",
      "65.5\n",
      "Sliced HR Aux Data Shape: (181, 180, 88, 45)\n",
      "-5.350948318234112\n",
      "(180, 88, 7)\n",
      "最大误差: 8.881784197001252e-16\n",
      "最大误差: 8.881784197001252e-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sun/miniconda3/lib/python3.12/site-packages/torch/nn/init.py:452: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "/home/sun/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sun/miniconda3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/150], Loss D: 0.5491, Loss G: 2.6563\n",
      "Epoch [2/150], Loss D: 0.3718, Loss G: 2.3943\n",
      "Epoch [3/150], Loss D: 0.4335, Loss G: 2.3101\n",
      "Epoch [4/150], Loss D: 0.4499, Loss G: 2.2412\n",
      "Epoch [5/150], Loss D: 0.4982, Loss G: 2.1757\n",
      "Epoch [6/150], Loss D: 0.5354, Loss G: 2.1236\n",
      "Epoch [7/150], Loss D: 0.5700, Loss G: 2.0774\n",
      "Epoch [8/150], Loss D: 0.5855, Loss G: 2.0370\n",
      "Epoch [9/150], Loss D: 0.6020, Loss G: 2.0085\n",
      "Epoch [10/150], Loss D: 0.6162, Loss G: 1.9934\n",
      "EarlyStopping: 1/20 epochs with no improvement.\n",
      "Epoch [11/150], Loss D: 0.6132, Loss G: 2.1545\n",
      "EarlyStopping: 2/20 epochs with no improvement.\n",
      "Epoch [12/150], Loss D: 0.6121, Loss G: 2.0932\n",
      "EarlyStopping: 3/20 epochs with no improvement.\n",
      "Epoch [13/150], Loss D: 0.6014, Loss G: 2.0476\n",
      "EarlyStopping: 4/20 epochs with no improvement.\n",
      "Epoch [14/150], Loss D: 0.5926, Loss G: 2.0126\n",
      "Epoch [15/150], Loss D: 0.5924, Loss G: 1.9781\n",
      "Epoch [16/150], Loss D: 0.5949, Loss G: 1.9527\n",
      "Epoch [17/150], Loss D: 0.5992, Loss G: 1.9199\n",
      "Epoch [18/150], Loss D: 0.6047, Loss G: 1.8836\n",
      "Epoch [19/150], Loss D: 0.6106, Loss G: 1.8681\n",
      "Epoch [20/150], Loss D: 0.6050, Loss G: 1.8422\n",
      "Epoch [21/150], Loss D: 0.6048, Loss G: 1.8156\n",
      "Epoch [22/150], Loss D: 0.6071, Loss G: 1.7920\n",
      "Epoch [23/150], Loss D: 0.6028, Loss G: 1.7676\n",
      "Epoch [24/150], Loss D: 0.6013, Loss G: 1.7487\n",
      "Epoch [25/150], Loss D: 0.6013, Loss G: 1.7327\n",
      "Epoch [26/150], Loss D: 0.6018, Loss G: 1.7215\n",
      "Epoch [27/150], Loss D: 0.6042, Loss G: 1.7125\n",
      "Epoch [28/150], Loss D: 0.6066, Loss G: 1.7061\n",
      "Epoch [29/150], Loss D: 0.6110, Loss G: 1.7020\n",
      "Epoch [30/150], Loss D: 0.6150, Loss G: 1.7007\n",
      "EarlyStopping: 1/20 epochs with no improvement.\n",
      "Epoch [31/150], Loss D: 0.6444, Loss G: 2.0050\n",
      "EarlyStopping: 2/20 epochs with no improvement.\n",
      "Epoch [32/150], Loss D: 0.6079, Loss G: 1.9706\n",
      "EarlyStopping: 3/20 epochs with no improvement.\n",
      "Epoch [33/150], Loss D: 0.5896, Loss G: 1.9572\n",
      "EarlyStopping: 4/20 epochs with no improvement.\n",
      "Epoch [34/150], Loss D: 0.5821, Loss G: 1.9697\n",
      "EarlyStopping: 5/20 epochs with no improvement.\n",
      "Epoch [35/150], Loss D: 0.5808, Loss G: 1.9704\n",
      "EarlyStopping: 6/20 epochs with no improvement.\n",
      "Epoch [36/150], Loss D: 0.5832, Loss G: 1.9641\n",
      "EarlyStopping: 7/20 epochs with no improvement.\n",
      "Epoch [37/150], Loss D: 0.5621, Loss G: 1.9593\n",
      "EarlyStopping: 8/20 epochs with no improvement.\n",
      "Epoch [38/150], Loss D: 0.5628, Loss G: 1.9635\n",
      "EarlyStopping: 9/20 epochs with no improvement.\n",
      "Epoch [39/150], Loss D: 0.5485, Loss G: 1.9755\n",
      "EarlyStopping: 10/20 epochs with no improvement.\n",
      "Epoch [40/150], Loss D: 0.5300, Loss G: 1.9838\n",
      "EarlyStopping: 11/20 epochs with no improvement.\n",
      "Epoch [41/150], Loss D: 0.5201, Loss G: 1.9928\n",
      "EarlyStopping: 12/20 epochs with no improvement.\n",
      "Epoch [42/150], Loss D: 0.5167, Loss G: 2.0052\n",
      "EarlyStopping: 13/20 epochs with no improvement.\n",
      "Epoch [43/150], Loss D: 0.5235, Loss G: 1.9981\n",
      "EarlyStopping: 14/20 epochs with no improvement.\n",
      "Epoch [44/150], Loss D: 0.5257, Loss G: 2.0090\n",
      "EarlyStopping: 15/20 epochs with no improvement.\n",
      "Epoch [45/150], Loss D: 0.5096, Loss G: 2.0087\n",
      "EarlyStopping: 16/20 epochs with no improvement.\n",
      "Epoch [46/150], Loss D: 0.4888, Loss G: 2.0244\n",
      "EarlyStopping: 17/20 epochs with no improvement.\n",
      "Epoch [47/150], Loss D: 0.4995, Loss G: 2.0224\n",
      "EarlyStopping: 18/20 epochs with no improvement.\n",
      "Epoch [48/150], Loss D: 0.4650, Loss G: 2.0170\n",
      "EarlyStopping: 19/20 epochs with no improvement.\n",
      "Epoch [49/150], Loss D: 0.4536, Loss G: 2.0381\n",
      "EarlyStopping: 20/20 epochs with no improvement.\n",
      "Early stopping triggered.\n",
      "Test MSE: 0.041998401284217834, Test MAE: 0.13048073649406433, Test R²: 0.9144102935600845, Test cc: [[1.         0.95663363]\n",
      " [0.95663363 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "epochs = 150\n",
    "batch_size = 12\n",
    "# Instantiate the modu\n",
    "\n",
    "\n",
    "# Define smoothing method\n",
    "smoothing_method = ModelTrainer(epochs, batch_size, OriginalRelationshipLearner(40), 1024).smooth_data_gaussian\n",
    "smoothing_method = None\n",
    "# Define modules\n",
    "#attention_module = AttentionModule(input_channels=40, output_channels=40)\n",
    "#senet_module = SqueezeExcitation(input_channels=40, reduction_ratio=8)\n",
    "\n",
    "# Train the baseline model without any additional module\n",
    "# Release GPU memory\n",
    "\n",
    "# Train the model with Attention\n",
    "model1 = ModelTrainer(epochs=epochs, batch_size=batch_size, relationship_learner=OriginalRelationshipLearner(40), relationship_output_channels=1024, smoothing_method=smoothing_method, attention='senet')\n",
    "train_losses_G1, train_losses_D1 = model1.train()\n",
    "preds1, trues1, r2_1 = model1.evaluate()\n",
    "torch.cuda.empty_cache()\n",
    "model2 = ModelTrainer(epochs=epochs, batch_size=batch_size, relationship_learner=OriginalRelationshipLearner(40), relationship_output_channels=1024, smoothing_method=smoothing_method, attention='senet', rand=26)\n",
    "train_losses_G2, train_losses_D2 = model2.train()\n",
    "preds2, trues2, r2_2 = model2.evaluate()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.upsampling_module.state_dict(), 'model11_upsampling_module.pth')\n",
    "torch.save(model2.upsampling_module.state_dict(), 'model12_upsampling_module.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
