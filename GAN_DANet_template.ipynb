{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from model import OriginalRelationshipLearner, Discriminator1, FlexibleUpsamplingModule, weights_init_normal, SSIM, TVLoss, PerceptualLoss\n",
    "from datasets import CustomDataset, load_data_with_augmentation\n",
    "import torch.nn.functional as F\n",
    "from utils import plot_results\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.ndimage import gaussian_filter, median_filter\n",
    "from scipy.signal import savgol_filter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from taylorDiagram import TaylorDiagram\n",
    "from torchvision import models\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "import copy\n",
    "def visualize_model(model, filename,x):\n",
    "    #x = torch.randn(input_size)\n",
    "    y = model(x)\n",
    "    dot = make_dot(y, params=dict(model.named_parameters()), show_attrs=True, show_saved=True)\n",
    "    dot.format = 'png'\n",
    "    dot.render(filename, cleanup=True)\n",
    "    print(f\"Model architecture saved as '{filename}.png'\")\n",
    "class ModelTrainer:\n",
    "    def __init__(self, epochs, batch_size, relationship_learner, relationship_output_channels, smoothing_method=None, attention=None, senet=None, rand=42):\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        #self.relationship_learner = relationship_learner\n",
    "        self.relationship_output_channels = relationship_output_channels\n",
    "        self.smoothing_method = smoothing_method\n",
    "        self.attention = attention\n",
    "        self.senet = senet\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.rand=rand\n",
    "        # Load and prepare data\n",
    "        [self.lr_grace_05,self.trend05], [self.lr_grace_025,self.trend25], self.hr_aux, self.grace_scaler_05, self.grace_scaler_025, self.aux_scalers = load_data_with_augmentation()\n",
    "        \n",
    "        # Apply data smoothing to hr_aux if smoothing_method is specified\n",
    "        if self.smoothing_method:\n",
    "            self.hr_aux = self.smoothing_method(self.hr_aux)\n",
    "        else:\n",
    "            self.hr_aux = self.hr_aux\n",
    "        \n",
    "        # Split data into training and testing sets\n",
    "        # Ensure data is sorted by time before splitting\n",
    "        split_index = int(len(self.lr_grace_05) * 0.8)  # 80% training, 20% testing\n",
    "        self.train_lr_grace_05, self.test_lr_grace_05 = self.lr_grace_05[:split_index], self.lr_grace_05[split_index:]\n",
    "        self.train_lr_grace_025, self.test_lr_grace_025 = self.lr_grace_025[:split_index], self.lr_grace_025[split_index:]\n",
    "        self.train_hr_aux, self.test_hr_aux = self.hr_aux[:split_index], self.hr_aux[split_index:]\n",
    "        self.train_lr_grace_05, self.test_lr_grace_05, self.train_lr_grace_025, self.test_lr_grace_025, self.train_hr_aux, self.test_hr_aux = train_test_split(\n",
    "            self.lr_grace_05, self.lr_grace_025, self.hr_aux, test_size=0.2, random_state=self.rand)\n",
    "        \n",
    "        # Create datasets and dataloaders\n",
    "        self.train_dataset = CustomDataset(self.train_lr_grace_05, self.train_lr_grace_025, self.train_hr_aux)\n",
    "        self.test_dataset = CustomDataset(self.test_lr_grace_05, self.test_lr_grace_025, self.test_hr_aux)\n",
    "        \n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=self.batch_size)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=self.batch_size)\n",
    "        \n",
    "        # Initialize models\n",
    "        #self.relationship_learner = self.relationship_learner.to(self.device)\n",
    "        self.discriminator = Discriminator1().to(self.device)\n",
    "        self.upsampling_module = FlexibleUpsamplingModule(input_channels=self.hr_aux.shape[-1]+1,attention_type=self.attention).to(self.device)\n",
    "        \n",
    "        self.flag=self.attention\n",
    "        self.attention=None\n",
    "        # Initialize optional modules\n",
    "        if self.attention:\n",
    "            self.attention_module = self.attention.to(self.device)\n",
    "        else:\n",
    "            self.attention_module = None\n",
    "            \n",
    "        if self.senet:\n",
    "            self.senet_module = self.senet.to(self.device)\n",
    "        else:\n",
    "            self.senet_module = None\n",
    "        \n",
    "        # Initialize weights\n",
    "        #self.relationship_learner.apply(weights_init_normal)\n",
    "        self.discriminator.apply(weights_init_normal)\n",
    "        self.upsampling_module.apply(weights_init_normal)\n",
    "        if self.attention_module:\n",
    "            self.attention_module.apply(weights_init_normal)\n",
    "        if self.senet_module:\n",
    "            self.senet_module.apply(weights_init_normal)\n",
    "        \n",
    "        # Optimizers\n",
    "        #self.optimizer_RL = optim.Adam(self.relationship_learner.parameters(), lr=0.0002)\n",
    "        hat_parameters = list(self.upsampling_module.parameters())\n",
    "\n",
    "        if self.attention_module:\n",
    "            hat_parameters += list(self.attention_module.parameters())\n",
    "\n",
    "        if self.senet_module:\n",
    "            hat_parameters += list(self.senet_module.parameters())\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer_D = optim.AdamW(self.discriminator.parameters(), lr=0.0004, betas=(0.5, 0.999), weight_decay=1e-4)\n",
    "        self.optimizer_U = optim.AdamW(hat_parameters, lr=0.0002, betas=(0.5, 0.999), weight_decay=1e-4)\n",
    "\n",
    "        # Learning Rate Schedulers\n",
    "        self.scheduler_D = CosineAnnealingWarmRestarts(self.optimizer_D, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "        self.scheduler_U = CosineAnnealingWarmRestarts(self.optimizer_U, T_0=10, T_mult=2, eta_min=1e-6)\n",
    "        \n",
    "        # Loss functions\n",
    "        self.adversarial_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        self.pixelwise_loss = torch.nn.MSELoss()\n",
    "        self.ssim_loss = SSIM(window_size=11, size_average=True).to(self.device)\n",
    "        self.tv_loss = TVLoss(weight=1e-5).to(self.device)\n",
    "        self.perceptual_loss = PerceptualLoss(use_gpu=torch.cuda.is_available())\n",
    "        #self.perceptual_loss = PerceptualLoss([1, 6, 11, 20], use_gpu=torch.cuda.is_available())\n",
    "    def smooth_data_gaussian(self, data, sigma=2):\n",
    "        return gaussian_filter(data, sigma=sigma)\n",
    "\n",
    "    def smooth_data_median(self, data, size=3):\n",
    "        return median_filter(data, size=size)\n",
    "\n",
    "    def smooth_data_savitzky_golay(self, data, window_length=5, polyorder=2):\n",
    "        return savgol_filter(data, window_length, polyorder)\n",
    "\n",
    "    def train(self):\n",
    "        train_losses_G = []\n",
    "        train_losses_D = []\n",
    "        patience = 20  # Early stopping patience\n",
    "        min_delta = 0  # Minimum change in monitored value to qualify as improvement\n",
    "        trigger_times = 0  # Counter for early stopping\n",
    "        best_loss = float('inf')\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss_G = 0\n",
    "            epoch_loss_D = 0\n",
    "\n",
    "            # Training phase\n",
    "            self.upsampling_module.train()\n",
    "            self.discriminator.train()\n",
    "            if self.attention_module:\n",
    "                self.attention_module.train()\n",
    "            if self.senet_module:\n",
    "                self.senet_module.train()\n",
    "\n",
    "            for lr_grace_05, lr_grace_025, hr_aux in self.train_loader:\n",
    "                lr_grace = F.interpolate(lr_grace_05, scale_factor=0.5, mode='bicubic', align_corners=False)\n",
    "                lr_grace, hr_aux = lr_grace.to(self.device), hr_aux.to(self.device)\n",
    "                lr_grace_025 = lr_grace_025.to(self.device)\n",
    "\n",
    "                # Combine lr_grace and downsampled hr_aux\n",
    "                downsampled_aux = F.interpolate(hr_aux, scale_factor=0.25, mode='bicubic', align_corners=False)\n",
    "                combined_input = torch.cat([lr_grace, downsampled_aux], dim=1)\n",
    "\n",
    "                # Learn relationship features\n",
    "                relationship_features = combined_input\n",
    "                # Apply attention or SENet if exists\n",
    "                if self.attention_module:\n",
    "                    relationship_features = self.attention_module(relationship_features)\n",
    "                elif self.senet_module:\n",
    "                    relationship_features = self.senet_module(relationship_features)\n",
    "\n",
    "                # Generate HR result using HAT module\n",
    "                hr_generated = self.upsampling_module(relationship_features)\n",
    "\n",
    "                # Discriminator training\n",
    "                self.optimizer_D.zero_grad()\n",
    "                real_output = self.discriminator(lr_grace_025)\n",
    "                fake_output = self.discriminator(hr_generated.detach())\n",
    "                real_labels = torch.ones_like(real_output, device=self.device)\n",
    "                fake_labels = torch.zeros_like(fake_output, device=self.device)\n",
    "\n",
    "                loss_D_real = self.adversarial_loss(real_output, real_labels)\n",
    "                loss_D_fake = self.adversarial_loss(fake_output, fake_labels)\n",
    "                loss_D = (loss_D_real + loss_D_fake) / 2\n",
    "                loss_D.backward()\n",
    "                self.optimizer_D.step()\n",
    "\n",
    "                # Generator training\n",
    "                self.optimizer_U.zero_grad()\n",
    "                fake_output = self.discriminator(hr_generated)\n",
    "                loss_G_adv = self.adversarial_loss(fake_output, real_labels)\n",
    "                loss_G_pixel = self.pixelwise_loss(hr_generated, lr_grace_025)\n",
    "                loss_G_ssim = 1 - self.ssim_loss(hr_generated, lr_grace_025)\n",
    "                loss_G_tv = self.tv_loss(hr_generated)\n",
    "                loss_G_perceptual = self.perceptual_loss(hr_generated, lr_grace_025)\n",
    "                loss_weight = epoch / self.epochs  # Linearly increase adversarial weight  HAT\n",
    "                loss_G = (1 - loss_weight) * loss_G_pixel + loss_weight * loss_G_adv + loss_G_tv + loss_G_perceptual\n",
    "                loss_G.backward()\n",
    "                self.optimizer_U.step()\n",
    "\n",
    "                epoch_loss_G += loss_G.item()\n",
    "                epoch_loss_D += loss_D.item()\n",
    "\n",
    "            # Average losses over the epoch\n",
    "            avg_epoch_loss_G = epoch_loss_G / len(self.train_loader)\n",
    "            avg_epoch_loss_D = epoch_loss_D / len(self.train_loader)\n",
    "\n",
    "            # Early Stopping Check\n",
    "            if avg_epoch_loss_G < best_loss - min_delta:\n",
    "                best_loss = avg_epoch_loss_G\n",
    "                trigger_times = 0\n",
    "                # Optionally save the best model state\n",
    "                torch.save(self.upsampling_module.state_dict(), 'best_model.pth')\n",
    "            else:\n",
    "                trigger_times += 1\n",
    "                print(f'EarlyStopping: {trigger_times}/{patience} epochs with no improvement.')\n",
    "                if trigger_times >= patience:\n",
    "                    print('Early stopping triggered.')\n",
    "                    # Load the best model state before stopping\n",
    "                    self.upsampling_module.load_state_dict(torch.load('best_model.pth'))\n",
    "                    return train_losses_G, train_losses_D\n",
    "\n",
    "            # Update the schedulers at the end of the epoch\n",
    "            self.scheduler_D.step()\n",
    "            self.scheduler_U.step()\n",
    "            if self.attention_module:\n",
    "                self.scheduler_A.step()\n",
    "            if self.senet_module:\n",
    "                self.scheduler_SE.step()\n",
    "\n",
    "            train_losses_G.append(avg_epoch_loss_G)\n",
    "            train_losses_D.append(avg_epoch_loss_D)\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/{self.epochs}], Loss D: {avg_epoch_loss_D:.4f}, Loss G: {avg_epoch_loss_G:.4f}')\n",
    "\n",
    "        # Load the best model at the end of training\n",
    "        self.upsampling_module.load_state_dict(torch.load('best_model.pth'))\n",
    "        return train_losses_G, train_losses_D\n",
    "\n",
    "    def evaluate(self):\n",
    "       # self.relationship_learner.eval()\n",
    "        self.upsampling_module.eval()\n",
    "        if self.attention_module:\n",
    "            self.attention_module.eval()\n",
    "        if self.senet_module:\n",
    "            self.senet_module.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = []\n",
    "            trues = []\n",
    "            bs=0\n",
    "            for lr_grace_05, lr_grace_025, hr_aux in self.test_loader:\n",
    "                \n",
    "                bs=bs+1\n",
    "                if bs==-1 :\n",
    "                    lr_grace_05, lr_grace_025, hr_aux = lr_grace_05.to(self.device), lr_grace_025.to(self.device), hr_aux.to(self.device)\n",
    "                    \n",
    "                    # Combine lr_grace and downsampled hr_aux\n",
    "                    combined_input = torch.cat([lr_grace_025, hr_aux], dim=1)\n",
    "\n",
    "                    # Learn relationship features\n",
    "                    #relationship_features = self.relationship_learner(combined_input)\n",
    "                    relationship_features = combined_input\n",
    "                    # Apply attention or SENet if exists\n",
    "                    if self.attention_module:\n",
    "                        relationship_features = self.attention_module(relationship_features)\n",
    "                    elif self.senet_module:\n",
    "                        relationship_features = self.senet_module(relationship_features)\n",
    "\n",
    "                    # Generate HR result using improved upsampling module\n",
    "                    hr_generated = self.upsampling_module(relationship_features)\n",
    "\n",
    "                    plot_results(lr_grace_05[0,0].cpu(), hr_generated[0,0].cpu(), lr_grace_025[0,0].cpu(), True)\n",
    "                # Save predictions and true values for metrics calculation\n",
    "                lr_grace_05, lr_grace_025, hr_aux = lr_grace_05.to(self.device), lr_grace_025.to(self.device), hr_aux.to(self.device)\n",
    "                lr_grace = F.interpolate(lr_grace_05, scale_factor=0.5, mode='bicubic', align_corners=False)\n",
    "                \n",
    "                # Combine lr_grace and downsampled hr_aux\n",
    "                downsampled_aux = F.interpolate(hr_aux, scale_factor=0.25, mode='bicubic', align_corners=False)\n",
    "                combined_input = torch.cat([lr_grace, downsampled_aux], dim=1)\n",
    "\n",
    "                # Learn relationship features\n",
    "                #relationship_features = self.relationship_learner(combined_input)\n",
    "                relationship_features = combined_input\n",
    "                # Apply attention or SENet if exists\n",
    "                if self.attention_module:\n",
    "                    relationship_features = self.attention_module(relationship_features)\n",
    "                elif self.senet_module:\n",
    "                    relationship_features = self.senet_module(relationship_features)\n",
    "\n",
    "                # Generate HR result using improved upsampling module\n",
    "                hr_generated = self.upsampling_module(relationship_features)\n",
    "\n",
    "                # Upsample lr_grace to create the ground truth for hr_generated\n",
    "                hr_grace_upsampled = lr_grace_025\n",
    "                preds.append(hr_generated.cpu().numpy())\n",
    "                trues.append(hr_grace_upsampled.cpu().numpy())\n",
    "\n",
    "            # Compute evaluation metrics\n",
    "            preds = np.concatenate(preds, axis=0).reshape(-1)\n",
    "            trues = np.concatenate(trues, axis=0).reshape(-1)\n",
    "\n",
    "            cc=np.corrcoef(trues, preds)\n",
    "            mse = mean_squared_error(trues, preds)\n",
    "            mae = mean_absolute_error(trues, preds)\n",
    "            r2 = r2_score(trues, preds)\n",
    "\n",
    "            print(f\"Test MSE: {mse}, Test MAE: {mae}, Test R²: {r2}, Test cc: {cc}\")\n",
    "\n",
    "        return preds, trues, r2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181, 90, 44)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/media/xy/data_op/ERA5/11/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate the modu\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Define smoothing method\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m smoothing_method \u001b[38;5;241m=\u001b[39m ModelTrainer(epochs, batch_size, OriginalRelationshipLearner(\u001b[38;5;241m40\u001b[39m), \u001b[38;5;241m1024\u001b[39m)\u001b[38;5;241m.\u001b[39msmooth_data_gaussian\n\u001b[1;32m      9\u001b[0m smoothing_method \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Define modules\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#attention_module = AttentionModule(input_channels=40, output_channels=40)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#senet_module = SqueezeExcitation(input_channels=40, reduction_ratio=8)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Train the model with Attention\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m, in \u001b[0;36mModelTrainer.__init__\u001b[0;34m(self, epochs, batch_size, relationship_learner, relationship_output_channels, smoothing_method, attention, senet, rand)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrand\u001b[38;5;241m=\u001b[39mrand\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Load and prepare data\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_grace_05,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrend05], [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_grace_025,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrend25], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhr_aux, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrace_scaler_05, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrace_scaler_025, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_scalers \u001b[38;5;241m=\u001b[39m load_data_with_augmentation()\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Apply data smoothing to hr_aux if smoothing_method is specified\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmoothing_method:\n",
      "File \u001b[0;32m/mnt/sdc/xy/data_op/gan/soca-gan/GAN-DANet/datasets.py:443\u001b[0m, in \u001b[0;36mload_data_with_augmentation\u001b[0;34m()\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data_with_augmentation\u001b[39m():\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# Load existing datasets\u001b[39;00m\n\u001b[0;32m--> 443\u001b[0m     [detrended, trend], [detrended25, trend25], smoothed_hr_aux, grace_scaler_05, grace_scaler_025, aux_scalers \u001b[38;5;241m=\u001b[39m load_data()\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# Define augmentation parameters\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     augmentation_factor \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# Number of augmentations\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/sdc/xy/data_op/gan/soca-gan/GAN-DANet/datasets.py:315\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m():\n\u001b[0;32m--> 315\u001b[0m     read_era()\n\u001b[1;32m    316\u001b[0m     lat05l\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m24.5\u001b[39m,\u001b[38;5;241m45.5\u001b[39m,\u001b[38;5;241m44\u001b[39m)\n\u001b[1;32m    317\u001b[0m     lon05l\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m65.5\u001b[39m,\u001b[38;5;241m109.5\u001b[39m,\u001b[38;5;241m90\u001b[39m)\n",
      "File \u001b[0;32m/mnt/sdc/xy/data_op/gan/soca-gan/GAN-DANet/datasets.py:206\u001b[0m, in \u001b[0;36mread_era\u001b[0;34m()\u001b[0m\n\u001b[1;32m    204\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATA_DIR\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/media/xy/data_op/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    205\u001b[0m era5_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mERA5/11/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m era5r,timer\u001b[38;5;241m=\u001b[39mNC_READ\u001b[38;5;241m.\u001b[39mreaddata(era5_path)\n\u001b[1;32m    207\u001b[0m era5\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(era5r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt2m\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    208\u001b[0m ET\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(era5r[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124me\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/mnt/sdc/xy/data_op/gan/soca-gan/GAN-DANet/NC_READ.py:16\u001b[0m, in \u001b[0;36mreaddata\u001b[0;34m(directory_path)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreaddata\u001b[39m(directory_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF:/ERA5/\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 16\u001b[0m     nc_files \u001b[38;5;241m=\u001b[39m [file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(directory_path) \u001b[38;5;28;01mif\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.nc\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.nc4\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# Initialize a dictionary to store datasets for each variable\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     datasets_dict \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/media/xy/data_op/ERA5/11/'"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "epochs = 150\n",
    "batch_size = 12\n",
    "# Instantiate the modu\n",
    "\n",
    "\n",
    "# Define smoothing method\n",
    "smoothing_method = ModelTrainer(epochs, batch_size, OriginalRelationshipLearner(40), 1024).smooth_data_gaussian\n",
    "smoothing_method = None\n",
    "# Define modules\n",
    "#attention_module = AttentionModule(input_channels=40, output_channels=40)\n",
    "#senet_module = SqueezeExcitation(input_channels=40, reduction_ratio=8)\n",
    "\n",
    "# Train the baseline model without any additional module\n",
    "# Release GPU memory\n",
    "\n",
    "# Train the model with Attention\n",
    "model1 = ModelTrainer(epochs=epochs, batch_size=batch_size, relationship_learner=OriginalRelationshipLearner(40), relationship_output_channels=1024, smoothing_method=smoothing_method, attention='senet')\n",
    "train_losses_G1, train_losses_D1 = model1.train()\n",
    "preds1, trues1, r2_1 = model1.evaluate()\n",
    "torch.cuda.empty_cache()\n",
    "model2 = ModelTrainer(epochs=epochs, batch_size=batch_size, relationship_learner=OriginalRelationshipLearner(40), relationship_output_channels=1024, smoothing_method=smoothing_method, attention='senet', rand=26)\n",
    "train_losses_G2, train_losses_D2 = model2.train()\n",
    "preds2, trues2, r2_2 = model2.evaluate()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model1.upsampling_module.state_dict(), 'model11_upsampling_module.pth')\n",
    "torch.save(model2.upsampling_module.state_dict(), 'model12_upsampling_module.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xyt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
